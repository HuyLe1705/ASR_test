import streamlit as st
import os
import tempfile
from faster_whisper import WhisperModel
import torch
import google.generativeai as genai
import time
from audio_recorder_streamlit import audio_recorder
import datetime
import difflib
from pyannote.audio import Pipeline
import torchaudio
import subprocess  # <-- TH√äM M·ªöI

# =========================
# C·∫§U H√åNH CHUNG
# =========================
st.set_page_config(layout="wide")
st.title("üé§ Voice Transcription (Whisper + Diarization + AI Corrector)")
st.markdown("Nh·∫≠n d·∫°ng gi·ªçng n√≥i, ph√¢n bi·ªát ng∆∞·ªùi n√≥i, v√† s·ª≠a l·ªói.")
log_filename = "log.txt"

# =========================
# C·∫§U H√åNH SIDEBAR
# =========================
with st.sidebar:
    st.header("C·∫•u h√¨nh Whisper")
    
    model_size = st.selectbox("Ch·ªçn model Whisper:", 
                              ["tiny", "base", "small", "medium", "large-v3"], 
                              index=2,
                              help="Model l·ªõn h∆°n (large-v3) ch√≠nh x√°c h∆°n nh∆∞ng ch·∫≠m h∆°n.")
    
    is_cuda_available = torch.cuda.is_available()
    device_option = st.radio("Thi·∫øt b·ªã x·ª≠ l√Ω (Whisper):", 
                             ["GPU (CUDA)", "CPU"], 
                             index=0 if is_cuda_available else 1,
                             disabled=not is_cuda_available,
                             help="GPU (CUDA) nhanh h∆°n r·∫•t nhi·ªÅu.")
    
    device = "cuda" if device_option == "GPU (CUDA)" else "cpu"
    compute_type = "float16" if device == "cuda" else "int8"
    st.info(f"Whisper d√πng: {device.upper()} ({compute_type})")

    beam_size = st.slider("Beam Size (Whisper):", 
                          min_value=1, 
                          max_value=10, 
                          value=5, 
                          help="TƒÉng gi√° tr·ªã ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c.")

    st.divider()
    
    st.header("C·∫•u h√¨nh AI s·ª≠a l·ªói")
    use_gemini = st.checkbox("S·ª≠a l·ªói ch√≠nh t·∫£ b·∫±ng AI", value=True)
    
    try:
        gemini_api_key = st.secrets["GEMINI_API_KEY"]
    except KeyError:
        gemini_api_key = None 
        
    try:
        hf_token = st.secrets["HF_TOKEN"]
    except KeyError:
        hf_token = None
        
    gemini_model_name = "gemini-2.5-flash" 

# =========================
# H√ÄM T·∫¢I MODEL (CACHE)
# =========================
@st.cache_resource
def load_whisper_model(model_size, device, compute_type):
    try:
        model = WhisperModel(model_size, device=device, compute_type=compute_type)
        return model
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i model Whisper: {e}.")
        return None

# (ƒê√É S·ª¨A L·ªñI TOKEN)
@st.cache_resource
def load_diarization_model(token):
    if not token:
        st.warning("Thi·∫øu HF_TOKEN trong secrets.toml. Kh√¥ng th·ªÉ ph√¢n bi·ªát ng∆∞·ªùi n√≥i.", icon="‚ö†Ô∏è")
        return None
    try:
        # T·∫£i pipeline v√† g·ª≠i token (S·ª¨A L·∫†I TH√ÄNH 'use_auth_token')
        pipeline = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=token  # <-- S·ª¨A L·∫†I CHO PHI√äN B·∫¢N 3.1.1
        )
        
        if torch.cuda.is_available():
            pipeline = pipeline.to(torch.device("cuda"))
            st.info("Pyannote: ƒê√£ chuy·ªÉn sang GPU (CUDA).")
        else:
            st.info("Pyannote: ƒêang d√πng CPU.")

        return pipeline
    except Exception as e:
        st.error(f"L·ªói t·∫£i model diarization: {e}. B·∫°n ƒë√£ ƒë·ªìng √Ω ƒëi·ªÅu kho·∫£n tr√™n Hugging Face ch∆∞a?", icon="üî•")
        return None

# T·∫£i c·∫£ 2 model
whisper_model = load_whisper_model(model_size, device, compute_type)
diarization_pipeline = load_diarization_model(hf_token)

if whisper_model:
    st.success(f"‚úÖ Model Whisper `{model_size}` ƒë√£ s·∫µn s√†ng.")
if diarization_pipeline:
    st.success("‚úÖ Model Ph√¢n bi·ªát ng∆∞·ªùi n√≥i (pyannote) ƒë√£ s·∫µn s√†ng.")

# =========================
# H√ÄM H·ªñ TR·ª¢ ƒê·ªäNH D·∫†NG TH·ªúI GIAN
# =========================
def format_timestamp(seconds_float):
    td = datetime.timedelta(seconds=seconds_float)
    total_seconds = int(td.total_seconds())
    hours, remainder = divmod(total_seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    milliseconds = int(td.microseconds / 1000)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}"

# =========================
# H√ÄM NH·∫¨N D·∫†NG (WHISPER)
# =========================
def run_whisper(model, file_path, beam_size):
    segments, info = model.transcribe(file_path, 
                                      language="vi", 
                                      vad_filter=True, 
                                      beam_size=beam_size,
                                      word_timestamps=True) 
    return list(segments)

# =========================
# H√ÄM S·ª¨A L·ªñI (GEMINI)
# =========================
def correct_spelling_with_gemini(text_to_correct, api_key):
    if not text_to_correct:
        return ""
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(gemini_model_name)
        
        prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia s·ª≠a l·ªói ch√≠nh t·∫£ ti·∫øng Vi·ªát.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† r√† so√°t v√† s·ª≠a c√°c l·ªói CH√çNH T·∫¢ (v√≠ d·ª•: sai 's'/'x', 'tr'/'ch', 'r'/'d'/'gi', d·∫•u h·ªèi/ng√£, v.v.) trong vƒÉn b·∫£n d∆∞·ªõi ƒë√¢y.
QUAN TR·ªåNG:
1. Ch·ªâ s·ª≠a c√°c t·ª´ b·ªã sai ch√≠nh t·∫£.
2. TUY·ªÜT ƒê·ªêI KH√îNG thay ƒë·ªïi c·∫•u tr√∫c c√¢u, kh√¥ng th√™m b·ªõt t·ª´.
3. Ph·∫£i gi·ªØ nguy√™n vƒÉn phong v√† c√°ch di·ªÖn ƒë·∫°t g·ªëc c·ªßa ng∆∞·ªùi n√≥i.
4. C√≥ th·ªÉ th√™m c√°c d·∫•u c√¢u (ph·∫©y, ch·∫•m, h·ªèi) n·∫øu n√≥ l√†m c√¢u r√µ nghƒ©a.
Ch·ªâ tr·∫£ v·ªÅ vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c s·ª≠a l·ªói, kh√¥ng th√™m b·∫•t k·ª≥ l·ªùi gi·∫£i th√≠ch n√†o.
---
VƒÉn b·∫£n g·ªëc:
{text_to_correct}
---
VƒÉn b·∫£n ƒë√£ s·ª≠a:
"""
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        st.toast(f"‚ö†Ô∏è L·ªói Gemini: {e}", icon="üî•")
        return text_to_correct

# =========================
# H√ÄM X√ÇY D·ª∞NG TIMELINE ƒê√É S·ª¨A
# =========================
def build_corrected_timeline_html(segment, speaker_label, corrected_segment_text):
    all_original_words = []
    if segment.words:
        all_original_words.extend(segment.words)
    
    original_text_list = [word.word for word in all_original_words]
    corrected_text_list = corrected_segment_text.split() 

    matcher = difflib.SequenceMatcher(None, original_text_list, corrected_text_list, autojunk=False)
    
    seg_start = format_timestamp(segment.start)
    seg_end = format_timestamp(segment.end)
    html = f"<div style='background-color:#222; border-left: 3px solid #00FF00; padding: 10px; border-radius: 5px; font-family: monospace; margin-bottom: 5px;'>"
    html += f"<p style='margin-bottom: 5px;'><strong style='color: cyan;'>[{speaker_label}]</strong> <strong>[{seg_start} -> {seg_end}]</strong>"
    
    try:
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal':
                for i in range(i1, i2):
                    word = all_original_words[i]
                    start = format_timestamp(word.start)
                    end = format_timestamp(word.end)
                    html += f" <span title='{start} -> {end}' style='cursor: help;'>{word.word}</span>"
            elif tag == 'replace':
                word_start_obj = all_original_words[i1]
                start = format_timestamp(word_start_obj.start)
                word_end_obj = all_original_words[i2-1]
                end = format_timestamp(word_end_obj.end)
                new_words = " ".join(corrected_text_list[j1:j2])
                html += f" <span title='{start} -> {end}' style='cursor: help; color: #00FF00; font-weight: bold;'>{new_words}</span>"
            elif tag == 'insert':
                new_words = " ".join(corrected_text_list[j1:j2])
                html += f" <span style='color: #999999; font-style: italic;'>{new_words}</span>"
            elif tag == 'delete':
                pass
    except (IndexError, KeyError):
        html += f" <span style='color: #00FF00;'>{corrected_segment_text}</span>"
    
    html += "</p></div>"
    return html

# =========================
# H√ÄM X·ª¨ L√ù CHUNG (ƒê√É S·ª¨A L·ªñI B·∫∞NG FFMPEG)
# =========================
def process_audio(audio_source_name, audio_bytes, use_gemini_flag, api_key, dia_pipeline, suffix=".wav"):
    
    if not whisper_model:
        st.error("Model Whisper ch∆∞a s·∫µn s√†ng.")
        return
    if not dia_pipeline:
        st.error("Model Ph√¢n bi·ªát ng∆∞·ªùi n√≥i (pyannote) ch∆∞a s·∫µn s√†ng.")
        return

    tmp_path_in = None
    tmp_path_wav = None
    
    try:
        with st.spinner(f"ƒêang x·ª≠ l√Ω {audio_source_name}..."):
            
            # --- B∆Ø·ªöC 1: T·∫†O FILE T·∫†M G·ªêC ---
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_in:
                tmp_in.write(audio_bytes)
                tmp_path_in = tmp_in.name

            # --- B∆Ø·ªöC 2: CHUY·ªÇN ƒê·ªîI SANG WAV B·∫∞NG FFMPEG (Gi·∫£i ph√°p d·ª©t ƒëi·ªÉm) ---
            st.info("‚è≥ B∆∞·ªõc 1/5: Chu·∫©n h√≥a √¢m thanh (FFmpeg)...")
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_out:
                tmp_path_wav = tmp_out.name
            
            try:
                cmd = [
                    "ffmpeg",
                    "-i", tmp_path_in,
                    "-ar", "16000",       # Resample to 16kHz
                    "-ac", "1",           # Set to 1 channel (mono)
                    "-map_metadata", "-1",
                    "-fflags", "+genpts",
                    "-y",                 # Overwrite output file
                    tmp_path_wav
                ]
                subprocess.run(cmd, check=True, capture_output=True, text=True) # Th√™m text=True
            except subprocess.CalledProcessError as e:
                st.error(f"L·ªói khi ch·∫°y FFmpeg ƒë·ªÉ chuy·ªÉn ƒë·ªïi file: {e.stderr}", icon="üî•")
                return # D·ª´ng l·∫°i n·∫øu kh√¥ng chuy·ªÉn ƒë·ªïi ƒë∆∞·ª£c

            # --- B∆Ø·ªöC 3: T·∫¢I AUDIO (Gi·ªù d√πng file wav ƒë√£ chu·∫©n h√≥a) ---
            st.info("‚è≥ B∆∞·ªõc 2/5: T·∫£i file √¢m thanh (torchaudio)...")
            try:
                waveform, sample_rate = torchaudio.load(tmp_path_wav)
            except Exception as e:
                st.error(f"L·ªói khi ƒë·ªçc file WAV ƒë√£ chuy·ªÉn ƒë·ªïi: {e}", icon="üî•")
                return

            # --- B∆Ø·ªöC 4: CH·∫†Y PH√ÇN BI·ªÜT NG∆Ø·ªúI N√ìI ---
            st.info("‚è≥ B∆∞·ªõc 3/5: Ph√¢n bi·ªát ng∆∞·ªùi n√≥i (pyannote)...")
            audio_data = {'waveform': waveform, 'sample_rate': sample_rate}
            diarization = dia_pipeline(audio_data)
            
            speaker_turns = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speaker_turns.append((turn.start, turn.end, speaker))

            # --- B∆Ø·ªöC 5: CH·∫†Y NH·∫¨N D·∫†NG (Whisper) ---
            st.info("‚è≥ B∆∞·ªõc 4/5: Nh·∫≠n d·∫°ng gi·ªçng n√≥i (Whisper)...")
            segment_list = run_whisper(whisper_model, tmp_path_wav, beam_size)

            if not segment_list:
                st.warning("‚ö†Ô∏è Whisper kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c gi·ªçng n√≥i.")
                return

            # --- B∆Ø·ªöC 6: MERGE, S·ª¨A L·ªñI V√Ä HI·ªÇN TH·ªä ---
            st.info("‚è≥ B∆∞·ªõc 5/5: G√°n nh√£n, s·ª≠a l·ªói v√† hi·ªÉn th·ªã...")
            
            st.markdown("### K·∫øt qu·∫£ g·ªëc (Whisper) v·ªõi Timeline")
            original_container = st.container(height=300)

            st.markdown("### K·∫øt qu·∫£ ƒë√£ s·ª≠a (Gemini) v·ªõi Timeline")
            corrected_container = st.container(height=300)
            
            original_html_full = ""
            corrected_html_full = ""
            all_raw_text = []
            all_corrected_text = []
            gemini_key_ok = (api_key is not None)

            for segment in segment_list:
                segment_midpoint = (segment.start + segment.end) / 2
                assigned_speaker = "UNKNOWN"
                for start, end, speaker in speaker_turns:
                    if start <= segment_midpoint <= end:
                        assigned_speaker = speaker
                        break
                
                raw_text = segment.text.strip()
                all_raw_text.append(raw_text)
                
                # --- X√¢y d·ª±ng HTML G·ªëc ---
                seg_start_f = format_timestamp(segment.start)
                seg_end_f = format_timestamp(segment.end)
                original_html = f"<div style='background-color:#222; border-left: 3px solid #FFD700; padding: 10px; border-radius: 5px; font-family: monospace; margin-bottom: 5px;'>"
                original_html += f"<p style='margin-bottom: 5px;'><strong style='color: cyan;'>[{assigned_speaker}]</strong> <strong>[{seg_start_f} -> {seg_end_f}]</strong>"
                
                if segment.words:
                    for word in segment.words:
                        word_start = format_timestamp(word.start)
                        word_end = format_timestamp(word.end)
                        original_html += f" <span title='{word_start} -> {word_end}' style='cursor: help;'>{word.word}</span>"
                else:
                    original_html += f" {segment.text}"
                original_html += "</p></div>"
                original_html_full += original_html
                
                # --- X·ª¨ L√ù GEMINI (N·∫æU B·∫¨T) ---
                if use_gemini_flag and gemini_key_ok:
                    corrected_text = correct_spelling_with_gemini(raw_text, api_key)
                    all_corrected_text.append(corrected_text)
                    corrected_html = build_corrected_timeline_html(segment, assigned_speaker, corrected_text)
                    corrected_html_full += corrected_html
                else:
                    all_corrected_text.append(raw_text)
                    corrected_html_full += original_html 

            # Hi·ªÉn th·ªã k·∫øt qu·∫£ (Batch)
            original_container.markdown(original_html_full, unsafe_allow_html=True)
            corrected_container.markdown(corrected_html_full, unsafe_allow_html=True)

            st.success("üéâ Ho√†n th√†nh x·ª≠ l√Ω to√†n b·ªô file!")

            # Ghi log (sau khi ƒë√£ x·ª≠ l√Ω h·∫øt)
            final_raw = " ".join(all_raw_text)
            final_corrected = " ".join(all_corrected_text)
            
            with open(log_filename, "a", encoding="utf-8") as log_file:
                log_file.write(f"--- [Ngu·ªìn: {audio_source_name} | {time.ctime()}] ---\n")
                log_file.write(f"[G·ªëc] {final_raw}\n")
                if use_gemini_flag and final_raw != final_corrected:
                    log_file.write(f"[S·ª≠a] {final_corrected}\n")
                log_file.write("\n")

    # D√íNG 348 C·ª¶A B·∫†N L√Ä D√íNG N√ÄY
    except Exception as e: 
        st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω √¢m thanh: {e}")
    
    finally:
        # X√≥a C·∫¢ HAI file t·∫°m
        if tmp_path_in and os.path.exists(tmp_path_in):
            os.remove(tmp_path_in)
        if tmp_path_wav and os.path.exists(tmp_path_wav):
            os.remove(tmp_path_wav)

# =========================
# GIAO DI·ªÜN CHIA THEO TAB (ƒê√É S·ª¨A: truy·ªÅn suffix)
# =========================
tab1, tab2 = st.tabs(["üìÅ T·∫£i file l√™n", "üî¥ Ghi √¢m tr·ª±c ti·∫øp"])

with tab1:
    st.header("T·∫£i file √¢m thanh")
    uploaded_file = st.file_uploader("Ch·ªçn t·ªáp √¢m thanh", type=["wav", "mp3", "m4a"], label_visibility="collapsed")
    
    if uploaded_file is not None and whisper_model:
        audio_bytes = uploaded_file.read()
        
        _ , file_suffix = os.path.splitext(uploaded_file.name)
        
        process_audio(f"File: {uploaded_file.name}", audio_bytes, use_gemini, gemini_api_key, diarization_pipeline, suffix=file_suffix)

with tab2:
    st.header("Ghi √¢m t·ª´ Micro")
    st.markdown("Nh·∫•n n√∫t b√™n d∆∞·ªõi ƒë·ªÉ b·∫Øt ƒë·∫ßu ghi √¢m. Nh·∫•n l·∫ßn n·ªØa ƒë·ªÉ d·ª´ng.")
    
    audio_bytes = audio_recorder(
        text="Nh·∫•n ƒë·ªÉ ghi √¢m",
        recording_color="#e84040",
        neutral_color="#6aa36f",
        icon_name="microphone",
        icon_size="3x",
    )
    
    if audio_bytes and whisper_model:
        st.audio(audio_bytes, format="audio/wav")
        process_audio("Ghi √¢m tr·ª±c ti·∫øp", audio_bytes, use_gemini, gemini_api_key, diarization_pipeline, suffix=".wav")

# --- N√öT T·∫¢I LOG (LU√îN HI·ªÇN TH·ªä) ---
if os.path.exists(log_filename):
    with open(log_filename, "r", encoding="utf-8") as f:
        log_data = f.read()
    st.sidebar.download_button("üì• T·∫£i to√†n b·ªô log.txt", 
                               data=log_data, 
                               file_name=log_filename)